name: CD

on:
  push:
    branches:
      - main

# Cancel in-progress runs when a new run is triggered
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  AWS_REGION: eu-south-2
  ECR_REPOSITORY: hermes

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker buildx build \
            --platform linux/arm64 \
            --push \
            --tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
            --tag $ECR_REGISTRY/$ECR_REPOSITORY:latest \
            --cache-from type=registry,ref=$ECR_REGISTRY/$ECR_REPOSITORY:buildcache \
            --cache-to type=registry,ref=$ECR_REGISTRY/$ECR_REPOSITORY:buildcache,mode=max \
            .
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

    outputs:
      image: ${{ steps.build-image.outputs.image }}

  deploy-infrastructure:
    name: Deploy Infrastructure with Terraform
    runs-on: ubuntu-latest
    needs: build-and-push

    defaults:
      run:
        working-directory: terraform

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init
        run: terraform init

      - name: Terraform Plan
        id: plan
        run: |
          terraform plan \
            -var="database_url=${{ secrets.DATABASE_URL }}" \
            -var="secret_key_base=${{ secrets.SECRET_KEY_BASE }}" \
            -var="anthropic_api_key=${{ secrets.ANTHROPIC_API_KEY }}" \
            -var="certificate_arn=${{ secrets.ACM_CERTIFICATE_ARN }}" \
            -var="key_name=${{ secrets.EC2_KEY_NAME }}" \
            -var="phx_host=${{ secrets.PHX_HOST }}" \
            -out=tfplan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        run: terraform apply -auto-approve tfplan

      - name: Get Terraform Outputs
        id: get-outputs
        run: |
          # Infrastructure outputs
          echo "target_group_arn=$(terraform output -raw target_group_arn)" >> $GITHUB_OUTPUT
          echo "target_group_name=$(terraform output -raw target_group_name)" >> $GITHUB_OUTPUT
          terraform output -json instance_ids > instance_ids.json
          echo "instance_ids=$(cat instance_ids.json)" >> $GITHUB_OUTPUT

          # Assets outputs
          echo "assets_bucket=$(terraform output -raw assets_bucket_name)" >> $GITHUB_OUTPUT
          echo "assets_url=$(terraform output -raw assets_url)" >> $GITHUB_OUTPUT
          echo "cloudfront_id=$(terraform output -raw cloudfront_distribution_id)" >> $GITHUB_OUTPUT

    outputs:
      target_group_arn: ${{ steps.get-outputs.outputs.target_group_arn }}
      target_group_name: ${{ steps.get-outputs.outputs.target_group_name }}
      instance_ids: ${{ steps.get-outputs.outputs.instance_ids }}
      assets_bucket: ${{ steps.get-outputs.outputs.assets_bucket }}
      assets_url: ${{ steps.get-outputs.outputs.assets_url }}
      cloudfront_id: ${{ steps.get-outputs.outputs.cloudfront_id }}

  upload-assets:
    name: Upload Static Assets to S3
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-infrastructure]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Extract static assets from Docker image
        env:
          IMAGE: ${{ needs.build-and-push.outputs.image }}
        run: |
          # Pull the ARM64 image (need to specify platform since runner is amd64)
          docker pull --platform linux/arm64 $IMAGE
          # Create a container from the image (don't run it)
          docker create --platform linux/arm64 --name assets-extractor $IMAGE
          # Copy static assets from the container
          docker cp assets-extractor:/app/lib/hermes-0.1.0/priv/static ./static
          # Remove the container
          docker rm assets-extractor
          # List extracted files
          ls -la ./static/

      - name: Upload assets to S3
        env:
          ASSETS_BUCKET: ${{ needs.deploy-infrastructure.outputs.assets_bucket }}
        run: |
          # Also copy static files from source (images, favicon, etc.)
          cp -r $GITHUB_WORKSPACE/priv/static/images ./static/ 2>/dev/null || true
          cp $GITHUB_WORKSPACE/priv/static/favicon.ico ./static/ 2>/dev/null || true
          cp $GITHUB_WORKSPACE/priv/static/robots.txt ./static/ 2>/dev/null || true

          # List what we're uploading
          echo "Static files to upload:"
          ls -la ./static/

          # Remove any .DS_Store files before uploading
          find ./static -name ".DS_Store" -delete

          # Sync assets to S3 with appropriate cache headers
          # Fingerprinted assets get long cache (1 year)
          aws s3 sync ./static/assets s3://$ASSETS_BUCKET/assets \
            --cache-control "public, max-age=31536000, immutable" \
            --exclude ".DS_Store" \
            --delete

          # Other static files get shorter cache (1 day)
          aws s3 sync ./static s3://$ASSETS_BUCKET \
            --exclude "assets/*" \
            --exclude ".DS_Store" \
            --cache-control "public, max-age=86400" \
            --delete

      - name: Invalidate CloudFront cache
        env:
          CLOUDFRONT_ID: ${{ needs.deploy-infrastructure.outputs.cloudfront_id }}
        run: |
          aws cloudfront create-invalidation \
            --distribution-id $CLOUDFRONT_ID \
            --paths "/*"

  run-migrations:
    name: Run Database Migrations
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-infrastructure]

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Run Database Migrations
        env:
          IMAGE: ${{ needs.build-and-push.outputs.image }}
          INSTANCE_IDS: ${{ needs.deploy-infrastructure.outputs.instance_ids }}
        run: |
          # Get first instance to run migrations
          INSTANCE_ID=$(echo "$INSTANCE_IDS" | jq -r '.[0]')

          echo "Running migrations on $INSTANCE_ID..."

          COMMAND_ID=$(aws ssm send-command \
            --instance-ids "$INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --parameters commands="[
              \"aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin ${{ steps.login-ecr.outputs.registry }}\",
              \"docker pull ${{ needs.build-and-push.outputs.image }}\",
              \"printf 'DATABASE_URL=${{ secrets.DATABASE_URL }}\\nSECRET_KEY_BASE=${{ secrets.SECRET_KEY_BASE }}\\nPOOL_SIZE=2\\n' > /tmp/migrate.env\",
              \"docker run --rm --env-file /tmp/migrate.env ${{ needs.build-and-push.outputs.image }} /app/bin/hermes eval 'Hermes.Release.migrate()'\",
              \"rm /tmp/migrate.env\"
            ]" \
            --query 'Command.CommandId' \
            --output text)

          echo "Migration Command ID: $COMMAND_ID"

          # Wait for migrations to complete
          aws ssm wait command-executed \
            --command-id "$COMMAND_ID" \
            --instance-id "$INSTANCE_ID" || true

          STATUS=$(aws ssm get-command-invocation \
            --command-id "$COMMAND_ID" \
            --instance-id "$INSTANCE_ID" \
            --query 'Status' \
            --output text)

          echo "Migration status: $STATUS"

          if [ "$STATUS" != "Success" ]; then
            echo "Migration output:"
            aws ssm get-command-invocation \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" \
              --query 'StandardOutputContent' \
              --output text
            echo "Migration errors:"
            aws ssm get-command-invocation \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" \
              --query 'StandardErrorContent' \
              --output text
            exit 1
          fi

          echo "Migrations completed successfully"

  rolling-deploy:
    name: Rolling Deployment
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-infrastructure, upload-assets, run-migrations]

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Rolling deploy to instances
        env:
          IMAGE: ${{ needs.build-and-push.outputs.image }}
          INSTANCE_IDS: ${{ needs.deploy-infrastructure.outputs.instance_ids }}
          TARGET_GROUP_ARN: ${{ needs.deploy-infrastructure.outputs.target_group_arn }}
          ASSETS_URL: ${{ needs.deploy-infrastructure.outputs.assets_url }}
        run: |
          echo "Starting rolling deployment..."

          # Get all instance IDs
          INSTANCES=$(echo "$INSTANCE_IDS" | jq -r '.[]')
          INSTANCE_COUNT=$(echo "$INSTANCE_IDS" | jq -r '. | length')

          echo "Deploying to $INSTANCE_COUNT instance(s)"

          INSTANCE_INDEX=0
          for INSTANCE_ID in $INSTANCES; do
            INSTANCE_INDEX=$((INSTANCE_INDEX + 1))
            echo ""
            echo "=========================================="
            echo "Deploying to instance $INSTANCE_INDEX of $INSTANCE_COUNT: $INSTANCE_ID"
            echo "=========================================="

            # Step 1: Deregister instance from target group (only if more than 1 instance)
            if [ "$INSTANCE_COUNT" -gt 1 ]; then
              echo "Step 1: Deregistering instance from target group..."
              aws elbv2 deregister-targets \
                --target-group-arn "$TARGET_GROUP_ARN" \
                --targets Id="$INSTANCE_ID"

              # Wait for connection draining (30 seconds as configured in ALB)
              echo "Waiting for connection draining (35 seconds)..."
              sleep 35
            else
              echo "Step 1: Skipping deregistration (single instance deployment)"
            fi

            # Step 2: Deploy to instance
            echo "Step 2: Deploying new version to instance..."
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters commands="[
                \"aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin ${{ steps.login-ecr.outputs.registry }}\",
                \"docker pull ${{ needs.build-and-push.outputs.image }}\",
                \"docker stop hermes || true\",
                \"docker rm hermes || true\",
                \"printf 'DATABASE_URL=${{ secrets.DATABASE_URL }}\\nSECRET_KEY_BASE=${{ secrets.SECRET_KEY_BASE }}\\nANTHROPIC_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}\\nPHX_HOST=${{ secrets.PHX_HOST }}\\nSTATIC_URL=${{ needs.deploy-infrastructure.outputs.assets_url }}\\nPHX_SERVER=true\\nPORT=4000\\nMIX_ENV=prod\\nPOOL_SIZE=5\\n' > /opt/hermes/.env\",
                \"docker run -d --name hermes --restart unless-stopped -p 4000:4000 --env-file /opt/hermes/.env ${{ needs.build-and-push.outputs.image }}\",
                \"sleep 15\",
                \"docker logs hermes 2>&1 | tail -50 || true\",
                \"curl -f http://localhost:4000/health || (docker logs hermes 2>&1 | tail -100 && exit 1)\"
              ]" \
              --query 'Command.CommandId' \
              --output text)

            echo "Deploy Command ID: $COMMAND_ID"

            # Wait for deployment to complete
            aws ssm wait command-executed \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" || true

            STATUS=$(aws ssm get-command-invocation \
              --command-id "$COMMAND_ID" \
              --instance-id "$INSTANCE_ID" \
              --query 'Status' \
              --output text)

            echo "Deploy status: $STATUS"

            if [ "$STATUS" != "Success" ]; then
              echo "Deployment failed on instance $INSTANCE_ID"
              echo "Command output:"
              aws ssm get-command-invocation \
                --command-id "$COMMAND_ID" \
                --instance-id "$INSTANCE_ID" \
                --query 'StandardOutputContent' \
                --output text
              echo "Command errors:"
              aws ssm get-command-invocation \
                --command-id "$COMMAND_ID" \
                --instance-id "$INSTANCE_ID" \
                --query 'StandardErrorContent' \
                --output text

              # Re-register instance if we deregistered it (even on failure for recovery)
              if [ "$INSTANCE_COUNT" -gt 1 ]; then
                echo "Re-registering instance to target group for recovery..."
                aws elbv2 register-targets \
                  --target-group-arn "$TARGET_GROUP_ARN" \
                  --targets Id="$INSTANCE_ID"
              fi
              exit 1
            fi

            # Step 3: Re-register instance with target group
            if [ "$INSTANCE_COUNT" -gt 1 ]; then
              echo "Step 3: Re-registering instance with target group..."
              aws elbv2 register-targets \
                --target-group-arn "$TARGET_GROUP_ARN" \
                --targets Id="$INSTANCE_ID"

              # Wait for instance to become healthy in target group
              echo "Waiting for instance to become healthy (checking every 10s, max 2 minutes)..."
              HEALTHY=false
              for i in {1..12}; do
                HEALTH_STATE=$(aws elbv2 describe-target-health \
                  --target-group-arn "$TARGET_GROUP_ARN" \
                  --targets Id="$INSTANCE_ID" \
                  --query 'TargetHealthDescriptions[0].TargetHealth.State' \
                  --output text)

                echo "Health check $i: $HEALTH_STATE"

                if [ "$HEALTH_STATE" == "healthy" ]; then
                  HEALTHY=true
                  break
                fi
                sleep 10
              done

              if [ "$HEALTHY" != "true" ]; then
                echo "WARNING: Instance did not become healthy within 2 minutes"
                echo "Continuing with deployment, but manual verification recommended"
              fi
            else
              echo "Step 3: Verifying instance health locally..."
              # For single instance, just verify the local health check passed (already done in deploy)
              echo "Instance health verified during deployment"
            fi

            echo "Instance $INSTANCE_ID deployed successfully"
          done

          echo ""
          echo "=========================================="
          echo "Rolling deployment completed successfully!"
          echo "=========================================="

  notify:
    name: Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-infrastructure, rolling-deploy]
    if: always()

    steps:
      - name: Deployment Status
        run: |
          if [ "${{ needs.rolling-deploy.result }}" == "success" ]; then
            echo "Deployment succeeded!"
            echo "Image: ${{ needs.build-and-push.outputs.image }}"
            echo ""
            echo "To rollback, redeploy a previous commit or manually deploy an older image"
          else
            echo "Deployment failed!"
            exit 1
          fi
